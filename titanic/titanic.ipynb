{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Titanic data challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "showPlots = False\n",
    "doGridSearch = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_df = pd.read_csv('data/titanic/train.csv')\n",
    "apply_df = pd.read_csv('data/titanic/test.csv')\n",
    "print(train_df[:3], '\\n')\n",
    "print(train_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test submissions\n",
    "Test files:\n",
    "* random prediction\n",
    "* survival for women and children only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createRandom, createWomenChildren = False, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if createRandom:\n",
    "    # Create dummy random output for first submission\n",
    "    id_df = apply_df['PassengerId']\n",
    "    random_df = pd.DataFrame(np.random.randint(low=0, high=2, size=(id_df.shape[0], 1)), columns=['Survived'])\n",
    "    result_df = pd.concat([id_df, random_df], axis=1)\n",
    "\n",
    "    # Save output to file\n",
    "    result_df.to_csv(\"results/result_random.csv\", index=False, float_format='%.0f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if createWomenChildren:\n",
    "    # Create dummy output with survival for women and children only\n",
    "    result_df = apply_df\n",
    "    result_df['Survived'] = ((result_df['Sex'] == 'female') | (result_df['Age'] < 16)).astype(int)\n",
    "\n",
    "    # Save output to file\n",
    "    result_df.to_csv(\"results/result_children_women.csv\", columns=['PassengerId', 'Survived'], index=False, float_format='%.0f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a general look\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find how many unique entries\n",
    "train_df.describe(include='O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many men\n",
    "train_df.loc[train_df['Sex'] == 'male'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a closer look at single features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "if showPlots:\n",
    "    g = sns.FacetGrid(train_df, col='Survived')\n",
    "    g.map(plt.hist, 'Age', bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if showPlots:\n",
    "    # grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\n",
    "    grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\n",
    "    grid.map(plt.hist, 'Age', alpha=.5, bins=20)\n",
    "    grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if showPlots:\n",
    "    grid = sns.FacetGrid(train_df, col='Embarked')\n",
    "#     grid = sns.FacetGrid(train_df, row='Embarked', size=2.2, aspect=1.6)\n",
    "    grid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep', \n",
    "             hue_order=['female', 'male'], order=[1,2,3])\n",
    "    grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if showPlots:\n",
    "#     grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})\n",
    "    grid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)\n",
    "    grid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None, order=['female', 'male'])\n",
    "    grid.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unusable data\n",
    "* drop PassengerId, Ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_full = train_df.drop([\"Survived\", \"PassengerId\", \"Ticket\"], axis=1)\n",
    "y_full = train_df[\"Survived\"]\n",
    "X_apply = apply_df.drop([\"PassengerId\", \"Ticket\"], axis=1)\n",
    "X_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "* number of family members\n",
    "* coarser binning for age\n",
    "* title from name\n",
    "* maybe add some products, sums etc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class MyFeatureAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_relatives=True, add_title=True, add_age_band=True):\n",
    "        self.add_title = add_title\n",
    "        self.add_relatives = add_relatives\n",
    "        self.add_age_band = add_age_band\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing to do\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if self.add_title:\n",
    "            pd.options.mode.chained_assignment = None  # creates slice-copy assignment warning\n",
    "            X['Title'] = X.Name.str.extract(r' ([A-Za-z]+)\\.', expand=False)\n",
    "            X['Title'].replace(['Lady', 'Countess', 'Capt', 'Col',\n",
    "                                'Don', 'Dr', 'Major', 'Rev', 'Sir',\n",
    "                                'Jonkheer', 'Dona'],\n",
    "                                'Rare', inplace=True)\n",
    "            X['Title'].replace('Mlle', 'Miss', inplace=True)\n",
    "            X['Title'].replace('Ms', 'Miss', inplace=True)\n",
    "            X['Title'].replace('Mme', 'Mrs', inplace=True)\n",
    "            pd.options.mode.chained_assignment = 'warn'\n",
    "\n",
    "        if self.add_relatives:\n",
    "#             X.loc[:, 'Family'] = X['SibSp'] + X['Parch']  # creates slice-copy assignment warning\n",
    "            Xtmp = pd.DataFrame(X['SibSp'] + X['Parch'], columns=['Family'])\n",
    "            X = pd.concat([X, Xtmp], axis=1)\n",
    "        \n",
    "        if self.add_age_band:\n",
    "            pd.options.mode.chained_assignment = None  # creates slice-copy assignment warning\n",
    "            def age_band(row, quantiles):\n",
    "                for i in range(len(quantiles)):\n",
    "                    if row['Age'] < quantiles[i]:\n",
    "                        return i-1\n",
    "                return len(quantiles)-1\n",
    "            nBins = 4\n",
    "            quantiles = [X['Age'].quantile(1.0 * q / nBins) for q in range(0, nBins+1)]\n",
    "            X['Age'] = X.apply(lambda row: age_band(row, quantiles), axis=1)\n",
    "            pd.options.mode.chained_assignment = 'warn'\n",
    "        \n",
    "        X = X.drop('Name', axis=1)\n",
    "        return X\n",
    "\n",
    "# feat_adder = MyFeatureAdder()\n",
    "# X_new = feat_adder.fit_transform(X_full)\n",
    "# print(X_new.head())\n",
    "# X_new.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform non-numeric labels to numeric ones\n",
    "* fill NaN values with sensible defaults\n",
    "* fill missing values with medians\n",
    "* integer labels for 'Sex', 'Embarked' and 'Cabin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class MyNumericizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_cabin_info=True, use_one_hot=False):\n",
    "        # no *args, **kargs to make use of BaseEstimator class\n",
    "        # other args can be steered later as hyperparameters\n",
    "        self.add_cabin_info = add_cabin_info\n",
    "        self.use_one_hot = use_one_hot\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing to do\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Sex\n",
    "        binarizer = LabelBinarizer(sparse_output=False)\n",
    "        X['Sex'] = binarizer.fit_transform(X['Sex'])\n",
    "        # Embarked\n",
    "        X['Embarked'].fillna('unknown', inplace=True)\n",
    "        if self.use_one_hot:\n",
    "            print('Warning: cross validation chokes on this!')\n",
    "            classes = ['C', 'S', 'Q', 'unknown']\n",
    "            X_embarked = pd.DataFrame(label_binarize(X['Embarked'], classes=classes),\n",
    "                                      columns=map(lambda s: 'Em_' + s, classes))\n",
    "            X = X.drop(['Embarked'], axis=1)\n",
    "            X = pd.concat([X, X_embarked], axis=1)\n",
    "        else:\n",
    "            encoder = LabelEncoder()\n",
    "            X['Embarked'] = encoder.fit_transform(X['Embarked'])\n",
    "        #cabin\n",
    "        encoder = LabelEncoder()\n",
    "        if self.add_cabin_info:\n",
    "            X['Cabin'].fillna('unknown', inplace=True)\n",
    "            X['Cabin'] = encoder.fit_transform(X['Cabin'])\n",
    "        else:\n",
    "            X = X.drop(['Cabin'], axis=1)\n",
    "        # Age\n",
    "        median = X['Age'].median()\n",
    "        X['Age'].fillna(median, inplace=True)\n",
    "        # Fare\n",
    "        median = X['Fare'].median()\n",
    "        X['Fare'].fillna(median, inplace=True)\n",
    "        # Title\n",
    "        if 'Title' in X.columns:\n",
    "            X['Title'].fillna('unknown', inplace=True)\n",
    "            X['Title'] = encoder.fit_transform(X['Title'])\n",
    "        \n",
    "        if X.isnull().any().any():\n",
    "            print('Warning: NaN values detected:')            \n",
    "            total_df = pd.DataFrame(np.full((X.shape[1], 1), \"/ \" + str(X.shape[0])))\n",
    "            total_df.index = X.columns\n",
    "            info_df = pd.concat([X.isnull().any(), X.isnull().sum(), total_df], axis=1)\n",
    "            info_df.columns = ['NaN?', '#NaN', 'Total']\n",
    "            print(info_df)\n",
    "            print('Single rows containing NaN values:')\n",
    "            print(X[(X.iloc[:,:].isnull()).any(axis=1)])\n",
    "            \n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "# print(X_full.head())\n",
    "# feat_adder = MyFeatureAdder()\n",
    "# X_new = feat_adder.fit_transform(X_full)\n",
    "# print(X_new.head())\n",
    "# numericizer = MyNumericizer()\n",
    "# X_new = numericizer.fit_transform(X_new)\n",
    "# print(X_new.head())\n",
    "# scaler = StandardScaler()\n",
    "# colNames = X_new.columns\n",
    "# X_new = scaler.fit_transform(X_new)\n",
    "# print(pd.DataFrame(X_new, columns=colNames).head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "#### Before transformation to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "if showPlots:\n",
    "    plt.figure()\n",
    "    X_full.hist(bins=50, figsize=(20, 15))\n",
    "    1  # prevent matlpotlib printout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After transformation to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_adder = MyFeatureAdder()\n",
    "my_num = MyNumericizer()\n",
    "X_plot = my_num.fit_transform(feat_adder.fit_transform(X_full))\n",
    "\n",
    "if showPlots:\n",
    "    plt.figure()\n",
    "    X_plot.hist(bins=50, figsize=(20, 15))\n",
    "    1  # prevent matlpotlib printout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if showPlots:\n",
    "    plt.figure()\n",
    "    scatter_matrix(pd.concat([X_plot, y_full], axis=1), figsize=(12, 8))\n",
    "    1  # prevent matlpotlib printout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "add_cabin_info = False  # adding this worsens accuracy\n",
    "add_relatives = True  # adding this improves accuracy\n",
    "add_title = True  # in current implementation worsens accuracy\n",
    "\n",
    "clfs = {}\n",
    "\n",
    "clfs['linear_svc'] = Pipeline((\n",
    "        ('MyFeatureAdder', MyFeatureAdder(add_relatives=add_relatives, add_title=add_title)),\n",
    "        ('MyNumericizer', MyNumericizer(add_cabin_info=add_cabin_info)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LinearSVC(C=1, loss='hinge')),\n",
    "        ))\n",
    "\n",
    "clfs['rbf_svc'] = Pipeline((\n",
    "        ('MyFeatureAdder', MyFeatureAdder(add_relatives=add_relatives, add_title=add_title)),\n",
    "        ('MyNumericizer', MyNumericizer(add_cabin_info=add_cabin_info)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', SVC(kernel='rbf', C=1)),\n",
    "        ))\n",
    "\n",
    "clfs['poly_svc'] = Pipeline((\n",
    "        ('MyFeatureAdder', MyFeatureAdder(add_relatives=add_relatives, add_title=add_title)),\n",
    "        ('MyNumericizer', MyNumericizer(add_cabin_info=add_cabin_info)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', SVC(kernel='poly', C=1)),\n",
    "        ))\n",
    "\n",
    "clfs['tree'] = Pipeline((\n",
    "        ('MyFeatureAdder', MyFeatureAdder(add_relatives=add_relatives, add_title=add_title)),\n",
    "        ('MyNumericizer', MyNumericizer(add_cabin_info=add_cabin_info)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', DecisionTreeClassifier()),\n",
    "        ))\n",
    "\n",
    "clfs['kNN'] = Pipeline((\n",
    "        ('MyFeatureAdder', MyFeatureAdder(add_relatives=add_relatives, add_title=add_title)),\n",
    "        ('MyNumericizer', MyNumericizer(add_cabin_info=add_cabin_info)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', KNeighborsClassifier(n_neighbors=3)),\n",
    "        ))\n",
    "\n",
    "clfs['gradBoost'] = Pipeline((\n",
    "        ('MyFeatureAdder', MyFeatureAdder(add_relatives=add_relatives, add_title=add_title)),\n",
    "        ('MyNumericizer', MyNumericizer(add_cabin_info=add_cabin_info)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', GradientBoostingClassifier(learning_rate=1.0, n_estimators=3, max_depth=2)),\n",
    "        ))\n",
    "\n",
    "clfs['randomForest'] = Pipeline((\n",
    "        ('MyFeatureAdder', MyFeatureAdder(add_relatives=add_relatives, add_title=add_title)),\n",
    "        ('MyNumericizer', MyNumericizer(add_cabin_info=add_cabin_info)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', RandomForestClassifier()),\n",
    "        ))\n",
    "\n",
    "clfs['log_reg'] = Pipeline((\n",
    "        ('MyFeatureAdder', MyFeatureAdder(add_relatives=add_relatives, add_title=add_title)),\n",
    "        ('MyNumericizer', MyNumericizer(add_cabin_info=add_cabin_info)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression()),\n",
    "        ))\n",
    "\n",
    "clfs['naive_bayes'] = Pipeline((\n",
    "        ('MyFeatureAdder', MyFeatureAdder(add_relatives=add_relatives, add_title=add_title)),\n",
    "        ('MyNumericizer', MyNumericizer(add_cabin_info=add_cabin_info)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', GaussianNB()),\n",
    "        ))\n",
    "\n",
    "clfs['perceptron'] = Pipeline((\n",
    "        ('MyFeatureAdder', MyFeatureAdder(add_relatives=add_relatives, add_title=add_title)),\n",
    "        ('MyNumericizer', MyNumericizer(add_cabin_info=add_cabin_info)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', Perceptron(max_iter=100)),\n",
    "        ))\n",
    "\n",
    "clfs['SGD'] = Pipeline((\n",
    "        ('MyFeatureAdder', MyFeatureAdder(add_relatives=add_relatives, add_title=add_title)),\n",
    "        ('MyNumericizer', MyNumericizer(add_cabin_info=add_cabin_info)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', SGDClassifier(max_iter=100)),\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Split training sample into train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.25, random_state=1337)\n",
    "# Reset the index to reach from 0 to n-1 to avoid NaN rows\n",
    "for x in [X_train, X_test, y_train, y_test]:\n",
    "    x.reset_index(drop=True, inplace=True)\n",
    "print('Split', X_full.shape[0], 'events into', X_train.shape[0], 'training and', X_test.shape[0], 'test events!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, clf in clfs.items():\n",
    "    clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_adder = MyFeatureAdder()\n",
    "X_new = feat_adder.fit_transform(X_full)\n",
    "numericizer = MyNumericizer()\n",
    "X_new = numericizer.fit_transform(X_new)\n",
    "\n",
    "coeff_df = pd.DataFrame(X_new.columns.delete(0))\n",
    "coeff_df.columns = ['Feature']\n",
    "coeff_df[\"Coeff\"] = pd.Series(clfs['log_reg'].named_steps['clf'].coef_[0])\n",
    "\n",
    "coeff_df.sort_values(by='Coeff', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model accuracy and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabulate\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def get_metrics(clf, name):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    accuracy = round(accuracy_score(y_test, y_pred), 3)\n",
    "    \n",
    "    cvs = cross_val_score(clf, X_train, y_train, cv=3, scoring='accuracy')\n",
    "    \n",
    "    y_train_pred = cross_val_predict(clf, X_train, y_train, cv=3)\n",
    "    \n",
    "    accuracy_train = round(accuracy_score(y_train, y_train_pred), 3)\n",
    "    \n",
    "    confusion = confusion_matrix(y_train, y_train_pred)\n",
    "    precision = precision_score(y_train, y_train_pred)\n",
    "    recall = recall_score(y_train, y_train_pred)\n",
    "    f1 = f1_score(y_train, y_train_pred)\n",
    "    \n",
    "    header = ['Algorithm', 'Accuracy\\nTest data', 'Accuracy\\nTrain data', 'Xval score\\nTrain data',\n",
    "              'Precision', 'Recall', 'F1', 'Confusion']\n",
    "    metrics = (name, accuracy, accuracy_train, cvs.mean(), precision, recall, f1, confusion)\n",
    "    \n",
    "    return {'header': header, 'metrics': metrics}\n",
    "\n",
    "headers = []\n",
    "metrics = []\n",
    "for name, clf in clfs.items():\n",
    "    thisMetrics = get_metrics(clf, name)\n",
    "    metrics.append(thisMetrics['metrics'])\n",
    "    headers = thisMetrics['header']\n",
    "\n",
    "metrics.sort(key=lambda x: x[1], reverse=True)\n",
    "print(tabulate.tabulate(metrics, headers=headers, tablefmt='grid'))\n",
    "\n",
    "top = metrics[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_vs_threshold(prec, rec, thresh, axes=None):\n",
    "    if axes is None:\n",
    "        fig, axes = plt.subplots(1,1)\n",
    "    axes.plot(thresh, prec[:-1], label='Precision')\n",
    "    axes.plot(thresh, rec[:-1], label='Recall')\n",
    "    axes.set_xlabel('Threshold')\n",
    "    axes.legend(loc='best')\n",
    "    axes.set_ylim([0, 1])\n",
    "\n",
    "\n",
    "def plot_precision_vs_recall(prec, rec, axes=None):\n",
    "    if axes is None:\n",
    "        fig, axes = plt.subplots(1,1)\n",
    "    axes.plot(rec, prec, 'b--', label='Precision')\n",
    "    axes.set_xlabel('Recall')\n",
    "    axes.set_ylabel('Precision')\n",
    "    axes.set_ylim([0, 1])\n",
    "\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None, axes=None):\n",
    "    if axes is None:\n",
    "        fig, axes = plt.subplots(1,1)\n",
    "    axes.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    axes.plot([0, 1], [0, 1], 'k--')\n",
    "    axes.axis([0, 1, 0, 1])\n",
    "    axes.set_xlabel('False Positive Rate')\n",
    "    axes.set_ylabel('True Positive Rate')\n",
    "    \n",
    "def plot_roc_curves(fprs, tprs, names):\n",
    "    plt.figure()\n",
    "    for fpr, tpr, name in zip(fprs, tprs, names):\n",
    "        plt.plot(fpr, tpr, label=name)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "\n",
    "fprs, tprs, names = [], [], []\n",
    "for idx, name in enumerate([item[0] for item in metrics]):\n",
    "    try:\n",
    "        y_scores = cross_val_predict(clfs[name], X_train, y_train, cv=3, method='decision_function')\n",
    "    except AttributeError:\n",
    "        print('No decision_function method for', name + '?')\n",
    "        continue\n",
    "\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)\n",
    "    fpr, tpr, roc_thresholds = roc_curve(y_train, y_scores)\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "    names.append(name)\n",
    "    \n",
    "    if showPlots:\n",
    "        plt.figure(idx, figsize=(9, 2))\n",
    "\n",
    "        plot_precision_recall_vs_threshold(precisions, recalls, thresholds, axes=plt.subplot(131))\n",
    "        plot_precision_vs_recall(precisions, recalls, axes=plt.subplot(132))\n",
    "        plot_roc_curve(fpr, tpr, axes=plt.subplot(133))\n",
    "\n",
    "        plt.suptitle(name)\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if showPlots:\n",
    "    plot_roc_curves(fprs, tprs, names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a grid search for hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "if doGridSearch:\n",
    "    parameters = {}\n",
    "    if top == 'gradBoost':\n",
    "        parameters = {\n",
    "                      'clf__learning_rate': [0.1, 0.2, 1.0],\n",
    "                      'clf__n_estimators': [3, 10, 100], \n",
    "                      'clf__max_depth': [2, 3, 4],\n",
    "                     }\n",
    "\n",
    "    parameters['MyFeatureAdder__add_title'] = (False, True)\n",
    "    parameters['MyFeatureAdder__add_relatives'] = (False, True)\n",
    "    parameters['MyNumericizer__add_cabin_info'] = (False, True)\n",
    "    parameters['MyFeatureAdder__add_age_band'] = (False, True)\n",
    "\n",
    "    grid_search = GridSearchCV(clfs[top], parameters, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(*sorted([(key + ': ' + str(val)) for key, val in grid_search.best_params_.items()]), sep='\\n')\n",
    "\n",
    "    top_optimized = grid_search.best_estimator_\n",
    "\n",
    "    print('Top score:', grid_search.best_score_)\n",
    "else:\n",
    "    print('Skip optimization!')\n",
    "    top_optimized = clfs[top]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the optimal estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topOptimizedMetrics = get_metrics(top_optimized, 'Optimized\\n' + top)\n",
    "topMetrics = get_metrics(clfs[top], 'Unoptimized\\n' + top)\n",
    "\n",
    "headers = topOptimizedMetrics['header']\n",
    "metrics = [topOptimizedMetrics['metrics'], topMetrics['metrics']]\n",
    "\n",
    "print(tabulate.tabulate(metrics, headers=headers, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = ['Optimized', 'Unoptimized']\n",
    "\n",
    "for idx, clf in enumerate((top_optimized, clfs[top])):\n",
    "    try:\n",
    "        y_scores = cross_val_predict(clf, X_train, y_train, cv=3, method='decision_function')\n",
    "    except AttributeError:\n",
    "        print(str(idx) + ': no decision_function method?')\n",
    "        continue\n",
    "    \n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)\n",
    "    fpr, tpr, roc_thresholds = roc_curve(y_train, y_scores)\n",
    "    \n",
    "    if showPlots:\n",
    "        plt.figure(idx, figsize=(9, 2))\n",
    "\n",
    "        plot_precision_recall_vs_threshold(precisions, recalls, thresholds, axes=plt.subplot(131))\n",
    "        plot_precision_vs_recall(precisions, recalls, axes=plt.subplot(132))\n",
    "        plot_roc_curve(fpr, tpr, axes=plt.subplot(133))\n",
    "\n",
    "        plt.suptitle(description[idx])\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = clfs[top].predict(X_apply)\n",
    "y_pred = top_optimized.predict(X_apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_pred = pd.DataFrame(apply_df['PassengerId'])\n",
    "y_pred = pd.DataFrame(y_pred, columns=['Survived'])\n",
    "result_df = pd.concat([id_pred, y_pred], axis=1)\n",
    "print(result_df.head())\n",
    "\n",
    "# Save output to file\n",
    "result_df.to_csv(\"results/result.csv\", index=False, float_format='%.0f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
